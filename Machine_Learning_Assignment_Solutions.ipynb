{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9cd1ae",
   "metadata": {},
   "source": [
    "# Machine Learning Intro Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eafe2a",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "**Explain the differences between AI, ML, Deep Learning (DL), and Data Science (DS).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d145f2",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI) is a broad field of computer science focused on building systems capable of performing tasks that typically require human intelligence, such as reasoning, planning, perception, and problem-solving. AI encompasses a wide range of technologies, from rule-based systems to advanced learning models.\n",
    "\n",
    "Machine Learning (ML) is a subfield of AI concerned with developing algorithms that enable computers to learn patterns and make decisions based on data, improving automatically with experience. ML removes the need for explicit programming by using data-driven approaches like regression, classification, and clustering.\n",
    "\n",
    "Deep Learning (DL) is a subset of ML that uses multi-layered artificial neural networks to model complex relationships and extract features from large volumes of data. DL is especially powerful for tasks involving images, speech, and text, using hierarchical learning to capture intricate patterns (e.g., image recognition, language translation).\n",
    "\n",
    "Data Science (DS) is an interdisciplinary field that combines statistics, programming, domain expertise, and data analysis to extract valuable insights and knowledge from structured and unstructured data. DS includes all aspects from data cleaning and preparation to statistical modeling and interpretation, leveraging AI, ML, and DL as tools within its methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256ecca",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**What are the types of machine learning? Describe each with one real-world example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe3f19",
   "metadata": {},
   "source": [
    "**Supervised Learning** involves training a model on labeled data, where each input has a known output. Example: Spam email detection, where emails are labeled as ‘spam’ or ‘not spam’, and the model learns to classify future emails based on these labels.\n",
    "\n",
    "**Unsupervised Learning** works with unlabeled data, identifying patterns or groupings without predefined outputs. Example: Customer segmentation uses clustering algorithms to divide customers into groups based on behavior, despite not knowing group labels beforehand.\n",
    "\n",
    "**Semi-supervised Learning** uses a combination of labeled and unlabeled data, useful when labeling data is expensive or time-consuming. Example: Photo categorization, where only some images are labeled but the system predicts labels for the unlabeled images after learning patterns from the labeled portion.\n",
    "\n",
    "**Reinforcement Learning** trains agents to make sequences of decisions using feedback in the form of rewards or penalties. Example: Game-playing AI agents (like AlphaGo) that learn optimal moves over time by interacting with the game environment and receiving scores for actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2d390",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "**Define overfitting, underfitting, and the bias-variance tradeoff in machine learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df2ba60",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model learns the training data—including its noise—to such an extent that it performs excellently on the training set but poorly on new, unseen data. Typically, overfitted models are too complex relative to the available data and fail to generalize.\n",
    "\n",
    "Underfitting happens when a model is too simplistic and fails to capture significant patterns in the training data, leading to poor performance on both the training and test sets. Underfitting usually arises from models with too few parameters or overly strong assumptions about data structure.\n",
    "\n",
    "The bias-variance tradeoff is the balancing act between accuracy and complexity. High bias means the model makes strong assumptions—leading to underfitting—while high variance means the model is too flexible and sensitive to noise—resulting in overfitting. The best models find a “sweet spot” where both bias and variance are minimized, achieving good predictiveness and generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af1e2e",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "**What are outliers in a dataset, and list three common techniques for handling them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c06a3",
   "metadata": {},
   "source": [
    "Outliers are data points that significantly deviate from the majority of values in a dataset. They may arise due to errors in data collection, natural variability, or genuine rare events. Outliers can skew statistical measures and impact model accuracy.\n",
    "\n",
    "Common techniques for handling outliers include:\n",
    "\n",
    "- **Removal:** Identify and delete outliers using statistical thresholds, such as values lying beyond 3 standard deviations from the mean or outside the interquartile range (IQR).\n",
    "\n",
    "- **Transformation:** Apply mathematical transformations (log, square root) to data to reduce the impact of outliers.\n",
    "\n",
    "- **Imputation:** Replace outliers with statistical measures like mean, median, or with predicted values from models trained on non-outlier data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7b8ab",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "**Explain the process of handling missing values and mention one imputation technique for numerical and one for categorical data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35ca12",
   "metadata": {},
   "source": [
    "Handling missing values starts with identifying missing entries (nulls, NaN, or invalid data), followed by understanding their cause (random or systematic). Next, the data analyst chooses a treatment method, including removal (dropping rows/columns), imputation (filling missing values), or prediction (using models to estimate missing values).\n",
    "\n",
    "For numerical data, one common imputation method is mean imputation: replacing missing values with the average of available values in the column. For categorical data, mode imputation is frequently used: filling missing entries with the most commonly occurring value in the category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9214d9",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "**Python: Create a synthetic imbalanced dataset and print class distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017b0bd",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.9,0.1], n_informative=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1, n_samples=1000, random_state=42)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print('Class distribution:', dict(zip(unique, counts)))\n",
    "```\n",
    "\n",
    "Output:\n",
    "-----------------------------------------------------------------------------------------------\n",
    "```py\n",
    "Class distribution: {np.int64(0): np.int64(898), np.int64(1): np.int64(102)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627be34",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "**Python: One-hot encode a list of colors and print the DataFrame.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f261bc",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "colors = ['Red', 'Green', 'Blue', 'Green', 'Red']\n",
    "df = pd.DataFrame({'Color': colors})\n",
    "encoded = pd.get_dummies(df, columns=['Color'])\n",
    "print(encoded)\n",
    "```\n",
    "Output\n",
    "---------------------------------------------------------------------------------------\n",
    "```python \n",
    "Color_Blue  Color_Green  Color_Red\n",
    "0       False        False       True\n",
    "1       False         True      False\n",
    "2        True        False      False\n",
    "3       False         True      False\n",
    "4       False        False       True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6bfd88",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "**Python: Simulate normal distribution, introduce missing values, mean-fill, plot histograms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5b95a",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = np.random.normal(loc=0, scale=1, size=1000)\n",
    "df = pd.DataFrame(samples, columns=['Sample'])\n",
    "missing_idx = np.random.choice(df.index, size=50, replace=False)\n",
    "df.loc[missing_idx, 'Sample'] = np.nan\n",
    "\n",
    "plt.hist(df['Sample'].dropna(), bins=30, alpha=0.5, label='Before Imputation')\n",
    "df['Sample'].fillna(df['Sample'].mean(), inplace=True)\n",
    "plt.hist(df['Sample'], bins=30, alpha=0.5, label='After Imputation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Output:\n",
    "---------------------------------------------------------------------------------------\n",
    "```python\n",
    "C:\\Users\\om\\AppData\\Local\\Temp\\ipykernel_7216\\2870388570.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  df['Sample'].fillna(df['Sample'].mean(), inplace=True)\n",
    "```\n",
    "\n",
    "<Figure size 640x480 with 1 Axes>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc750f5",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "**Python: Min-Max scaling for given numbers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d579016",
   "metadata": {},
   "source": [
    "```py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[2], [5], [10], [15], [20]])\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(data)\n",
    "print('Scaled data:', scaled.flatten())\n",
    "```\n",
    "Output:\n",
    "-------------------------------------------------------------\n",
    "```py \n",
    "Scaled data: [0.         0.16666667 0.44444444 0.72222222 1.        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68f1e4",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "**Data Preparation Plan for Retail Fraud Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a6a34",
   "metadata": {},
   "source": [
    "As a data scientist, preparing this retail customer transaction data involves:\n",
    "\n",
    "- **Step 1: Data Cleaning**: Remove duplicates/irrelevant entries.\n",
    "- **Step 2: Handle Missing Data**: For ages, use mean imputation (numerical) or `unknown` for extremes.\n",
    "- **Step 3: Outlier Treatment**: Detect via Z-score/IQR, remove or use Winsorization.\n",
    "- **Step 4: Imbalance Handling**: Use resampling (SMOTE), downsampling, or cost-sensitive learning.\n",
    "- **Step 5: Encode Categoricals**: One-hot encoding for payment method.\n",
    "- **Step 6: Feature Engineering**: New features, Min-Max scale numbers.\n",
    "- **Step 7: Model Step**: Train/test split.\n",
    "\n",
    "Example Python template:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# df = pd.read_csv('transactions.csv')\n",
    "# df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "# df = df[df['TransactionAmount'].between(df['TransactionAmount'].quantile(0.05), df['TransactionAmount'].quantile(0.95))]\n",
    "# ohe = OneHotEncoder(sparse=False)\n",
    "# encoded = ohe.fit_transform(df[['PaymentMethod']])\n",
    "# smote = SMOTE()\n",
    "# X_res, y_res = smote.fit_resample(X, y)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
