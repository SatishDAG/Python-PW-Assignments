{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1cE4gq5B0r_"
   },
   "source": [
    "# Supervised Classification: Decision Trees, SVM, and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eU7C2X4tCF59"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8U8_0n1gCGwE"
   },
   "source": [
    "## Question 1: What is Information Gain, and how is it used in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M079-06CI6D"
   },
   "source": [
    "**Information Gain** is a metric used in decision tree algorithms to measure the reduction in uncertainty or randomness (entropy) in a dataset after it is split on a particular feature. üå≥\n",
    "\n",
    "### How It's Used in Decision Trees\n",
    "\n",
    "The primary use of Information Gain is to decide the best feature to split the data on at each node of the tree. The process is as follows:\n",
    "\n",
    "1.  **Calculate Initial Entropy**: At a given node, the entropy of the dataset is calculated to measure its initial impurity.\n",
    "2.  **Calculate Conditional Entropy**: For each feature, the dataset is hypothetically split. The weighted average entropy of the resulting child nodes (the conditional entropy) is calculated.\n",
    "3.  **Calculate Information Gain**: The Information Gain for that feature is the initial entropy minus the conditional entropy.\n",
    "    \n",
    "    `Information Gain = Entropy(parent) - Weighted Average Entropy(children)`\n",
    "\n",
    "4.  **Select Best Feature**: The algorithm repeats this for all features. The feature that results in the **highest Information Gain** is chosen as the splitting criterion for that node because it creates the most homogeneous (purest) child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5t7d_uNCSfD"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P2vG63zCTu2"
   },
   "source": [
    "## Question 2: What is the difference between Gini Impurity and Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9x7L_6BCWEg"
   },
   "source": [
    "**Gini Impurity** and **Entropy** are both metrics used to measure the level of impurity or disorder within a node of a decision tree. The goal of a split is to reduce this impurity. While they often produce very similar results, they differ in their calculation and sensitivity.\n",
    "\n",
    "| Feature | Gini Impurity | Entropy |\n",
    "|---|---|---|\n",
    "| **Concept** | Measures the probability of misclassifying a randomly chosen element from the node. | Measures the amount of uncertainty or randomness in the node's data. |\n",
    "| **Calculation** | Faster to compute as it avoids logarithmic calculations. `Gini = 1 - Œ£(p_i)¬≤` | Slower to compute due to the logarithm. `Entropy = -Œ£(p_i * log‚ÇÇ(p_i))` |\n",
    "| **Range** | 0 to 0.5 (for binary classification) | 0 to 1 (for binary classification) |\n",
    "| **Behavior** | Tends to isolate the most frequent class in its own branch. | Tends to produce slightly more balanced trees. |\n",
    "| **Use Case** | The default criterion in scikit-learn's `DecisionTreeClassifier` due to its computational efficiency. | Used in algorithms like ID3 and C4.5. Can be slightly more effective for datasets with more complex class distributions. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB6l0yLMCZGc"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBfK7Q-9CaA2"
   },
   "source": [
    "## Question 3: What is Pre-Pruning in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D26m3t-aCc9r"
   },
   "source": [
    "**Pre-pruning** is a technique used to prevent a decision tree from overfitting the training data by **stopping its growth early**. üõë\n",
    "\n",
    "Instead of building a complete tree and then trimming it back (post-pruning), pre-pruning sets conditions or thresholds that stop a branch from splitting further. If a proposed split does not meet these conditions, it is rejected, and the current node becomes a leaf node.\n",
    "\n",
    "Common pre-pruning strategies include:\n",
    "\n",
    "- **`max_depth`**: Limiting the maximum depth the tree can grow to.\n",
    "- **`min_samples_split`**: Setting the minimum number of data points required in a node before it can be split.\n",
    "- **`min_samples_leaf`**: Defining the minimum number of data points that must exist in a leaf node after a split.\n",
    "- **`max_leaf_nodes`**: Limiting the total number of leaf nodes in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1y9i778Ch8g"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3K_223ACiqc"
   },
   "source": [
    "## Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load a sample dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 1. Initialize the Decision Tree Classifier with Gini Impurity\n",
    "# 'gini' is the default criterion, but we specify it for clarity.\n",
    "gini_tree = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "# 2. Train the model\n",
    "gini_tree.fit(X, y)\n",
    "\n",
    "# 3. Get and print the feature importances\n",
    "feature_importances = gini_tree.feature_importances_\n",
    "\n",
    "print(\"Feature Importances for the Iris Dataset (using Gini Impurity):\")\n",
    "print(\"----------------------------------------------------------------\")\n",
    "for i, feature_name in enumerate(iris.feature_names):\n",
    "    print(f\"{feature_name}: {feature_importances[i]:.4f}\")\n",
    "\n",
    "Output:\n",
    "--------------------------------------------------------------------------------\n",
    "Feature Importances for the Iris Dataset (using Gini Impurity):\n",
    "----------------------------------------------------------------\n",
    "sepal length (cm): 0.0133\n",
    "sepal width (cm): 0.0000\n",
    "petal length (cm): 0.5641\n",
    "petal width (cm): 0.4226"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0zV0rXPCp5J"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5M41wG6Cqnf"
   },
   "source": [
    "## Question 5: What is a Support Vector Machine (SVM)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7c1T6i6CtVb"
   },
   "source": [
    "A **Support Vector Machine (SVM)** is a powerful and versatile supervised machine learning algorithm used for both **classification** and **regression** tasks. ü§ñ\n",
    "\n",
    "The core idea of an SVM, especially for classification, is to find the **optimal hyperplane** that best separates a dataset into classes.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Hyperplane**: This is the decision boundary. In a 2D space, it's a line; in a 3D space, it's a plane; in higher dimensions, it's a hyperplane.\n",
    "- **Margin**: The margin is the distance between the hyperplane and the nearest data points from each class. SVM aims to **maximize this margin**.\n",
    "- **Support Vectors**: These are the data points that lie closest to the hyperplane and define the margin. They are the critical elements of the dataset, as removing them would alter the position of the optimal hyperplane.\n",
    "\n",
    "By maximizing the margin, SVM finds a decision boundary that is robust and generalizes well to new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hG9U7r5CCzcL"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZqP39RSC0sA"
   },
   "source": [
    "## Question 6: What is the Kernel Trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qR8p-5v3C3hB"
   },
   "source": [
    "The **Kernel Trick** is a clever mathematical technique that allows SVMs to solve **non-linear classification problems**. üßô‚Äç‚ôÇÔ∏è\n",
    "\n",
    "### The Problem\n",
    "Many real-world datasets are not linearly separable, meaning a straight line (or plane) cannot effectively separate the classes.\n",
    "\n",
    "### The Solution\n",
    "The Kernel Trick enables the SVM to work in a higher-dimensional space **without actually transforming the data**. It does this by using a **kernel function**.\n",
    "\n",
    "Instead of:\n",
    "1.  Transforming the data into a much higher dimension (which is computationally expensive).\n",
    "2.  Finding a hyperplane there.\n",
    "\n",
    "The kernel function directly computes the dot product of the data points as if they were in that higher-dimensional space. This allows the SVM to find a non-linear decision boundary in the original, lower-dimensional space.\n",
    "\n",
    "Common kernel functions include:\n",
    "- **Linear**\n",
    "- **Polynomial**\n",
    "- **Radial Basis Function (RBF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5kF_q0lC6iT"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI8_T-JPC7wP"
   },
   "source": [
    "## Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load and split the dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 2. Train and evaluate the SVM with a Linear Kernel\n",
    "linear_svm = SVC(kernel='linear', random_state=42)\n",
    "linear_svm.fit(X_train, y_train)\n",
    "linear_predictions = linear_svm.predict(X_test)\n",
    "linear_accuracy = accuracy_score(y_test, linear_predictions)\n",
    "\n",
    "# 3. Train and evaluate the SVM with an RBF Kernel\n",
    "rbf_svm = SVC(kernel='rbf', random_state=42)\n",
    "rbf_svm.fit(X_train, y_train)\n",
    "rbf_predictions = rbf_svm.predict(X_test)\n",
    "rbf_accuracy = accuracy_score(y_test, rbf_predictions)\n",
    "\n",
    "# 4. Compare the accuracies\n",
    "print(\"SVM Accuracy Comparison on the Wine Dataset:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(f\"Accuracy with Linear Kernel: {linear_accuracy:.4f}\")\n",
    "print(f\"Accuracy with RBF Kernel:    {rbf_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "Output:\n",
    "------------------------------------------------------------\n",
    "SVM Accuracy Comparison on the Wine Dataset:\n",
    "---------------------------------------------\n",
    "Accuracy with Linear Kernel: 0.9444\n",
    "Accuracy with RBF Kernel:    0.6667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0G03mR0DE3R"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z62B_QhRDFS3"
   },
   "source": [
    "## Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9Vn-KxFDHkM"
   },
   "source": [
    "The **Na√Øve Bayes classifier** is a simple yet effective probabilistic classification algorithm based on **Bayes' Theorem**. It calculates the probability of an observation belonging to a particular class based on the values of its features.\n",
    "\n",
    "### Why is it called \"Na√Øve\"?\n",
    "\n",
    "The classifier is called \"na√Øve\" because it makes a very strong, and often unrealistic, assumption about the data: **the assumption of class-conditional independence**. ü§î\n",
    "\n",
    "This means that the algorithm assumes that all the features are **independent of one another**, given the class.\n",
    "\n",
    "**Example:** When classifying an email as `spam` or `not spam`:\n",
    "- A Na√Øve Bayes classifier would assume that the presence of the word \"viagra\" is completely independent of the presence of the word \"money\".\n",
    "- In reality, these words often appear together in spam emails, so they are not independent.\n",
    "\n",
    "Despite this \"na√Øve\" assumption, the classifier works surprisingly well in many real-world applications, particularly for text classification and medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEsvX9sRDLd0"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM30kF8UDM08"
   },
   "source": [
    "## Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9yBvYVFDOrU"
   },
   "source": [
    "The main difference between the various types of Na√Øve Bayes classifiers lies in the assumptions they make about the distribution of the feature data.\n",
    "\n",
    "### 1. Gaussian Na√Øve Bayes\n",
    "- **Use Case**: Used for **continuous features**.\n",
    "- **Assumption**: It assumes that the features for each class follow a **Gaussian (normal) distribution**.\n",
    "- **Example**: It's suitable for data like patient height, weight, temperature, or the feature values in the Iris or Breast Cancer datasets.\n",
    "\n",
    "### 2. Multinomial Na√Øve Bayes\n",
    "- **Use Case**: Used for **discrete features**, typically representing counts or frequencies.\n",
    "- **Assumption**: It models the data based on multinomial distribution.\n",
    "- **Example**: Its most common application is in **text classification**, where the features are the frequency of each word appearing in a document (e.g., word counts).\n",
    "\n",
    "### 3. Bernoulli Na√Øve Bayes\n",
    "- **Use Case**: Used for **binary/boolean features** (i.e., features that are either present (1) or absent (0)).\n",
    "- **Assumption**: It models data based on the Bernoulli distribution.\n",
    "- **Example**: Also used in text classification, but instead of word counts, it only considers whether a word appears in a document or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t1-g4n_DRL5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D60YVn-DR-5"
   },
   "source": [
    "## Question 10: Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# 2. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Initialize and train the Gaussian Na√Øve Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions on the test set\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# 5. Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Gaussian Na√Øve Bayes Classifier on Breast Cancer Dataset\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "Output:\n",
    "-------------------------------------------------------------------\n",
    "Gaussian Na√Øve Bayes Classifier on Breast Cancer Dataset\n",
    "--------------------------------------------------------\n",
    "Model Accuracy: 0.9415"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
